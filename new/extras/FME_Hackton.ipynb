{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvBPDkSE9EXZ"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision\n",
        "!pip install pandas numpy scikit-learn matplotlib seaborn\n",
        "!pip install tqdm\n",
        "!pip install xgboost lightgbm\n",
        "!pip install -q dice-ml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import shap\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "R2VyWtvp9L02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Complete TabCBM Installation for Google Colab with Python 3.8\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "\n",
        "# Step 1: Install condacolab (this will restart the runtime)\n",
        "print(\"üì¶ Installing condacolab...\")\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "\n",
        "# NOTE: After runtime restarts, run the cell below"
      ],
      "metadata": {
        "id": "g5p3gSe1BuHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 2: Complete TabCBM Installation with Python 3.8\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üöÄ TabCBM Installation for Google Colab\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Step 1: Create Python 3.8 environment\n",
        "print(\"\\n[1/6] üîß Creating Python 3.8 environment...\")\n",
        "!conda create -n tabcbm_env python=3.8 -y -q\n",
        "\n",
        "# Step 2: Install all dependencies\n",
        "print(\"\\n[2/6] üì• Installing dependencies (this takes ~3-5 minutes)...\")\n",
        "!conda run -n tabcbm_env pip install -q \\\n",
        "  anndata cachetools captum cmake colorama commonmark \\\n",
        "  configobj configparser dill h5py joblib keras Keras-Preprocessing \\\n",
        "  numba numexpr \"numpy<1.25\" \"pandas<2.1\" Pillow \\\n",
        "  prettytable pyaml pytorch-lightning pytorch-tabnet PyYAML \\\n",
        "  scanpy scikit-image \"scikit-learn>=1.0.2\" scikit-learn-extra \\\n",
        "  scipy setuptools-scm tensorflow\n",
        "\n",
        "# Step 3: Clone TabCBM repository\n",
        "print(\"\\n[3/6] üì¶ Cloning TabCBM repository...\")\n",
        "!rm -rf /tmp/tabcbm\n",
        "!git clone -q https://github.com/mateoespinosa/tabcbm.git /tmp/tabcbm\n",
        "\n",
        "# Step 4: Fix sklearn dependency issue\n",
        "print(\"\\n[4/6] üîß Fixing sklearn dependency issue...\")\n",
        "!cd /tmp/tabcbm && sed -i '/^sklearn/d' requirements.txt\n",
        "\n",
        "# Step 5: Fix concepts_xai import issues\n",
        "print(\"\\n[5/6] üîß Fixing concepts_xai import issues...\")\n",
        "\n",
        "fix_imports_script = \"\"\"\n",
        "import os\n",
        "import sys\n",
        "\n",
        "fixed_files = []\n",
        "init_file = '/tmp/tabcbm/tabcbm/concepts_xai/__init__.py'\n",
        "\n",
        "# Fix the main __init__.py\n",
        "try:\n",
        "    with open(init_file, 'r') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    content = content.replace('from concepts_xai.', 'from tabcbm.concepts_xai.')\n",
        "    content = content.replace('import concepts_xai.', 'import tabcbm.concepts_xai.')\n",
        "\n",
        "    with open(init_file, 'w') as f:\n",
        "        f.write(content)\n",
        "    fixed_files.append(init_file)\n",
        "except Exception as e:\n",
        "    print(f\"Error fixing {init_file}: {e}\")\n",
        "\n",
        "# Fix all other Python files in concepts_xai\n",
        "for root, dirs, files in os.walk('/tmp/tabcbm/tabcbm/concepts_xai'):\n",
        "    for file in files:\n",
        "        if file.endswith('.py') and file != '__init__.py':\n",
        "            filepath = os.path.join(root, file)\n",
        "            try:\n",
        "                with open(filepath, 'r') as f:\n",
        "                    content = f.read()\n",
        "\n",
        "                original_content = content\n",
        "                content = content.replace('from concepts_xai.', 'from tabcbm.concepts_xai.')\n",
        "                content = content.replace('import concepts_xai.', 'import tabcbm.concepts_xai.')\n",
        "\n",
        "                if content != original_content:\n",
        "                    with open(filepath, 'w') as f:\n",
        "                        f.write(content)\n",
        "                    fixed_files.append(filepath)\n",
        "            except Exception as e:\n",
        "                pass\n",
        "\n",
        "print(f\"Fixed {len(fixed_files)} files\")\n",
        "\"\"\"\n",
        "\n",
        "with open('/tmp/fix_imports.py', 'w') as f:\n",
        "    f.write(fix_imports_script)\n",
        "\n",
        "!python /tmp/fix_imports.py\n",
        "\n",
        "# Step 6: Install TabCBM\n",
        "print(\"\\n[6/6] ‚öôÔ∏è  Installing TabCBM...\")\n",
        "!conda run -n tabcbm_env pip install --no-deps -e /tmp/tabcbm -q\n",
        "\n",
        "# Verification\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ Verifying Installation\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "verification_script = \"\"\"\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "try:\n",
        "    # Test imports\n",
        "    import tabcbm\n",
        "    from tabcbm.models.tabcbm import TabCBM\n",
        "    import tensorflow as tf\n",
        "    import numpy as np\n",
        "\n",
        "    print(\"\\\\n‚úÖ TabCBM successfully installed!\")\n",
        "    print(f\"‚úÖ Python version: {sys.version.split()[0]}\")\n",
        "    print(f\"‚úÖ TensorFlow version: {tf.__version__}\")\n",
        "    print(f\"‚úÖ NumPy version: {np.__version__}\")\n",
        "    print(\"\\\\nüéâ Installation completed successfully!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\\\n‚ùå Error during verification: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    sys.exit(1)\n",
        "\"\"\"\n",
        "\n",
        "with open('/tmp/verify_installation.py', 'w') as f:\n",
        "    f.write(verification_script)\n",
        "\n",
        "!conda run -n tabcbm_env python /tmp/verify_installation.py\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìù Usage Instructions\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "To use TabCBM in your code, create a Python script and run it with:\n",
        "\n",
        "    !conda run -n tabcbm_env python your_script.py\n",
        "\n",
        "Example: See CELL 3 below for a working example!\n",
        "\"\"\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "0AEJaWTJBx0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "TZe8EaYy9PUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('dataset.csv')\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"\\nColumn names:\\n{df.columns.tolist()}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "display(df.head())\n",
        "print(f\"\\nData types:\")\n",
        "print(df.dtypes)\n",
        "print(f\"\\nTarget variable distribution:\")\n",
        "print(df['target_variable'].value_counts())"
      ],
      "metadata": {
        "id": "0j2EXGlG9Rve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify feature types\n",
        "target_col = 'target_variable'\n",
        "id_col = 'id'\n",
        "\n",
        "# Separate features\n",
        "feature_cols = [col for col in df.columns if col not in [target_col, id_col]]\n",
        "\n",
        "# Identify categorical and numerical features\n",
        "categorical_features = []\n",
        "numerical_features = []\n",
        "\n",
        "for col in feature_cols:\n",
        "    if df[col].dtype == 'object' or df[col].nunique() < 10:\n",
        "        categorical_features.append(col)\n",
        "    else:\n",
        "        numerical_features.append(col)\n",
        "\n",
        "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
        "print(f\"Numerical features ({len(numerical_features)}): {numerical_features}\")"
      ],
      "metadata": {
        "id": "rfYc02L_9ayr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data\n",
        "X = df[feature_cols].copy()\n",
        "y = df[target_col].values\n",
        "\n",
        "# Encode categorical features\n",
        "label_encoders = {}\n",
        "for col in categorical_features:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Standardize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
        "X_test[numerical_features] = scaler.transform(X_test[numerical_features])\n",
        "\n",
        "print(f\"Train set: {X_train.shape}, Test set: {X_test.shape}\")\n",
        "print(f\"Train target distribution: {np.bincount(y_train)}\")\n",
        "print(f\"Test target distribution: {np.bincount(y_test)}\")"
      ],
      "metadata": {
        "id": "lLBozYgO9jAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "classifier = RandomForestClassifier(\n",
        "    n_estimators=65,\n",
        "    max_depth=35,\n",
        "    random_state=42,\n",
        "    min_samples_split=6,\n",
        "    min_samples_leaf=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate classifier\n",
        "y_pred_train = classifier.predict(X_train)\n",
        "y_pred_test = classifier.predict(X_test)\n",
        "\n",
        "print(f\"\\nTrain Accuracy: {accuracy_score(y_train, y_pred_train):.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_test):.4f}\")\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_test))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "cm = confusion_matrix(y_test, y_pred_test)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D0WMvWpJ9mOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Generating SHAP Feature Importance ---\")\n",
        "explainer = shap.TreeExplainer(classifier)\n",
        "\n",
        "# Sample test data for faster computation\n",
        "shap_values = explainer.shap_values(X_test)\n"
      ],
      "metadata": {
        "id": "5wcREag0Al8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Feature Importance ---\")\n",
        "importances = classifier.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'feature': feature_cols,\n",
        "    'importance': importances\n",
        "}).sort_values(by='importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Most Important Features:\")\n",
        "print(feature_importance_df.head(10))\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "top_n = min(20, len(feature_importance_df))\n",
        "sns.barplot(\n",
        "    x='importance',\n",
        "    y='feature',\n",
        "    data=feature_importance_df.head(top_n),\n",
        "    palette='viridis'\n",
        ")\n",
        "plt.title(f'Top {top_n} Feature Importance from Random Forest', fontsize=14)\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Features')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E5x84IHVAuyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 3: Examine the uploaded dataset\n",
        "# ============================================================================\n",
        "\n",
        "examine_script = \"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/dataset.csv')\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üìä Dataset Overview\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\\\nüîç Dataset Shape: {df.shape}\")\n",
        "print(f\"   ‚Ä¢ Rows: {df.shape[0]}\")\n",
        "print(f\"   ‚Ä¢ Columns: {df.shape[1]}\")\n",
        "\n",
        "print(f\"\\\\nüìã Column Names:\")\n",
        "for i, col in enumerate(df.columns, 1):\n",
        "    print(f\"   {i}. {col} (dtype: {df[col].dtype})\")\n",
        "\n",
        "print(f\"\\\\nüìà First Few Rows:\")\n",
        "print(df.head(10))\n",
        "\n",
        "print(f\"\\\\nüî¢ Basic Statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "print(f\"\\\\n‚ùì Missing Values:\")\n",
        "missing = df.isnull().sum()\n",
        "if missing.sum() > 0:\n",
        "    print(missing[missing > 0])\n",
        "else:\n",
        "    print(\"   No missing values!\")\n",
        "\n",
        "print(f\"\\\\nüéØ Data Types Summary:\")\n",
        "print(df.dtypes.value_counts())\n",
        "\n",
        "# Check for potential target columns\n",
        "potential_targets = ['target', 'label', 'class', 'y', 'outcome', 'diagnosis', 'prediction']\n",
        "target_col = None\n",
        "for col in df.columns:\n",
        "    if col.lower() in potential_targets:\n",
        "        target_col = col\n",
        "        print(f\"\\\\n‚úÖ Potential target column found: '{target_col}'\")\n",
        "        print(f\"   Unique values: {df[target_col].nunique()}\")\n",
        "        print(f\"   Value counts:\")\n",
        "        print(df[target_col].value_counts())\n",
        "        break\n",
        "\n",
        "if target_col is None:\n",
        "    print(f\"\\\\n‚ö†Ô∏è  No obvious target column found.\")\n",
        "    print(f\"   Assuming last column ('{df.columns[-1]}') is the target.\")\n",
        "    print(f\"   Unique values: {df[df.columns[-1]].nunique()}\")\n",
        "    print(f\"   Value counts:\")\n",
        "    print(df[df.columns[-1]].value_counts())\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*70)\n",
        "\"\"\"\n",
        "\n",
        "with open('/tmp/examine_dataset.py', 'w') as f:\n",
        "    f.write(examine_script)\n",
        "\n",
        "!conda run -n tabcbm_env python /tmp/examine_dataset.py"
      ],
      "metadata": {
        "id": "NCmSdXGICyF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# FINAL FIX: Handle Tuple Return from TabCBM\n",
        "# ============================================================================\n",
        "\n",
        "tabcbm_training_script = \"\"\"\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from tabcbm.models.tabcbm import TabCBM\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üöÄ TabCBM Training Pipeline\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ============================================================================\n",
        "# 1-4. Data Loading and Preprocessing\n",
        "# ============================================================================\n",
        "print(\"\\\\n[1/7] üìä Loading dataset...\")\n",
        "df = pd.read_csv('/content/dataset.csv')\n",
        "print(f\"  ‚Ä¢ Dataset shape: {df.shape}\")\n",
        "\n",
        "TARGET_COLUMN = df.columns[-1]\n",
        "print(f\"  ‚Ä¢ Target column: '{TARGET_COLUMN}'\")\n",
        "\n",
        "X = df.drop(columns=[TARGET_COLUMN])\n",
        "y = df[TARGET_COLUMN]\n",
        "\n",
        "print(\"\\\\n[2/7] üîß Preprocessing data...\")\n",
        "\n",
        "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
        "if len(categorical_cols) > 0:\n",
        "    print(f\"  ‚Ä¢ Found {len(categorical_cols)} categorical columns\")\n",
        "    for col in categorical_cols:\n",
        "        le = LabelEncoder()\n",
        "        X[col] = le.fit_transform(X[col].astype(str))\n",
        "\n",
        "if y.dtype == 'object' or y.dtype.name == 'category':\n",
        "    print(f\"  ‚Ä¢ Encoding target variable...\")\n",
        "    le_target = LabelEncoder()\n",
        "    y = le_target.fit_transform(y)\n",
        "    n_classes = len(le_target.classes_)\n",
        "    print(f\"  ‚Ä¢ Classes: {list(le_target.classes_)}\")\n",
        "else:\n",
        "    n_classes = len(np.unique(y))\n",
        "\n",
        "print(f\"  ‚Ä¢ Number of classes: {n_classes}\")\n",
        "\n",
        "if X.isnull().any().any():\n",
        "    print(f\"  ‚Ä¢ Filling missing values...\")\n",
        "    X = X.fillna(X.mean())\n",
        "\n",
        "print(\"\\\\n[3/7] ‚úÇÔ∏è  Splitting data...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"  ‚Ä¢ Training samples: {X_train.shape[0]}\")\n",
        "print(f\"  ‚Ä¢ Testing samples: {X_test.shape[0]}\")\n",
        "\n",
        "print(\"\\\\n[4/7] üìè Standardizing features...\")\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train).astype(np.float32)\n",
        "X_test_scaled = scaler.transform(X_test).astype(np.float32)\n",
        "\n",
        "n_features = X_train_scaled.shape[1]\n",
        "print(f\"  ‚Ä¢ Number of features: {n_features}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 5. Build TabCBM Model\n",
        "# ============================================================================\n",
        "print(\"\\\\n[5/7] üèóÔ∏è  Building TabCBM model...\")\n",
        "\n",
        "N_CONCEPTS = min(4, n_features)\n",
        "LATENT_DIMS = N_CONCEPTS\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS_PRETRAIN = 30\n",
        "EPOCHS_TRAIN = 50\n",
        "\n",
        "print(f\"  ‚Ä¢ Number of concepts: {N_CONCEPTS}\")\n",
        "print(f\"  ‚Ä¢ Latent dimensions: {LATENT_DIMS}\")\n",
        "print(f\"  ‚Ä¢ Input features: {n_features}\")\n",
        "\n",
        "features_to_concepts = tf.keras.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=(n_features,)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(LATENT_DIMS, activation='relu')\n",
        "], name='features_to_concepts')\n",
        "\n",
        "concepts_to_labels = tf.keras.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=(N_CONCEPTS,)),\n",
        "    tf.keras.layers.Dense(16, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(8, activation='relu'),\n",
        "    tf.keras.layers.Dense(n_classes, activation='softmax')\n",
        "], name='concepts_to_labels')\n",
        "\n",
        "g_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=(LATENT_DIMS,)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(n_features, activation='linear')\n",
        "], name='reconstruction_model')\n",
        "\n",
        "print(\"  ‚úÖ Model architecture defined\")\n",
        "\n",
        "# ============================================================================\n",
        "# 6. Self-Supervised Pre-training\n",
        "# ============================================================================\n",
        "print(\"\\\\n[6/7] üîÑ Self-supervised pre-training...\")\n",
        "\n",
        "tabcbm_pretrain = TabCBM(\n",
        "    features_to_concepts_model=features_to_concepts,\n",
        "    concepts_to_labels_model=concepts_to_labels,\n",
        "    g_model=g_model,\n",
        "    latent_dims=LATENT_DIMS,\n",
        "    n_concepts=N_CONCEPTS,\n",
        "    n_supervised_concepts=0,\n",
        "    coherence_reg_weight=0.1,\n",
        "    diversity_reg_weight=15.0,\n",
        "    feature_selection_reg_weight=2.0,\n",
        "    gate_estimator_weight=0.1,\n",
        "    concept_generator_units=[32],\n",
        "    self_supervised_mode=True\n",
        ")\n",
        "\n",
        "tabcbm_pretrain.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        ")\n",
        "\n",
        "print(f\"  ‚Ä¢ Pre-training for {EPOCHS_PRETRAIN} epochs...\")\n",
        "\n",
        "history_pretrain = tabcbm_pretrain.fit(\n",
        "    X_train_scaled,\n",
        "    y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=EPOCHS_PRETRAIN,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "print(f\"  ‚úÖ Pre-training complete (Final loss: {history_pretrain.history['loss'][-1]:.4f})\")\n",
        "\n",
        "# ============================================================================\n",
        "# 7. Supervised Training\n",
        "# ============================================================================\n",
        "print(\"\\\\n[7/7] üéì Supervised training...\")\n",
        "\n",
        "tabcbm = TabCBM(\n",
        "    features_to_concepts_model=features_to_concepts,\n",
        "    concepts_to_labels_model=concepts_to_labels,\n",
        "    latent_dims=LATENT_DIMS,\n",
        "    n_concepts=N_CONCEPTS,\n",
        "    n_supervised_concepts=0,\n",
        "    coherence_reg_weight=0.1,\n",
        "    diversity_reg_weight=15.0,\n",
        "    feature_selection_reg_weight=2.0,\n",
        "    concept_generator_units=[32],\n",
        "    self_supervised_mode=False,\n",
        "    concept_generators=tabcbm_pretrain.concept_generators,\n",
        "    prior_masks=tabcbm_pretrain.feature_probabilities\n",
        ")\n",
        "\n",
        "tabcbm.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        ")\n",
        "\n",
        "print(f\"  ‚Ä¢ Training for {EPOCHS_TRAIN} epochs...\")\n",
        "\n",
        "history = tabcbm.fit(\n",
        "    X_train_scaled,\n",
        "    y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=EPOCHS_TRAIN,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# 8. Evaluation - FIXED: Handle Tuple Return\n",
        "# ============================================================================\n",
        "print(\"\\\\n\" + \"=\"*70)\n",
        "print(\"üìä Model Evaluation\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\\\nMaking predictions...\")\n",
        "\n",
        "y_pred_list = []\n",
        "batch_size_pred = 256\n",
        "\n",
        "for i in range(0, len(X_test_scaled), batch_size_pred):\n",
        "    batch = X_test_scaled[i:i+batch_size_pred]\n",
        "    pred_batch = tabcbm(batch, training=False)\n",
        "\n",
        "    # ‚úÖ FIX: Handle tuple return\n",
        "    if isinstance(pred_batch, tuple):\n",
        "        # TabCBM returns (predictions, concept_scores)\n",
        "        pred_batch = pred_batch[0]  # Get predictions only\n",
        "\n",
        "    # Convert to numpy\n",
        "    if hasattr(pred_batch, 'numpy'):\n",
        "        pred_batch = pred_batch.numpy()\n",
        "    else:\n",
        "        pred_batch = np.array(pred_batch)\n",
        "\n",
        "    y_pred_list.append(pred_batch)\n",
        "\n",
        "# Concatenate all predictions\n",
        "y_pred_probs = np.concatenate(y_pred_list, axis=0)\n",
        "\n",
        "print(f\"  ‚Ä¢ Prediction shape: {y_pred_probs.shape}\")\n",
        "\n",
        "# Handle different output formats\n",
        "if len(y_pred_probs.shape) == 1:\n",
        "    y_pred_probs = np.column_stack([1 - y_pred_probs, y_pred_probs])\n",
        "elif y_pred_probs.shape[0] != len(X_test):\n",
        "    y_pred_probs = y_pred_probs.T\n",
        "\n",
        "print(f\"  ‚Ä¢ Final shape: {y_pred_probs.shape}\")\n",
        "\n",
        "# Get predicted classes\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\\\n‚úÖ Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\\\nüìã Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "print(f\"\\\\nüî¢ Confusion Matrix:\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "\n",
        "# Calculate per-class accuracy\n",
        "print(f\"\\\\nüìä Per-Class Performance:\")\n",
        "for i in range(n_classes):\n",
        "    class_mask = y_test == i\n",
        "    if class_mask.sum() > 0:\n",
        "        class_acc = (y_pred[class_mask] == i).sum() / class_mask.sum()\n",
        "        print(f\"   Class {i}: {class_acc:.4f} ({class_acc*100:.2f}%)\")\n",
        "\n",
        "# ============================================================================\n",
        "# 9. Concept Analysis & Interpretability\n",
        "# ============================================================================\n",
        "print(\"\\\\n\" + \"=\"*70)\n",
        "print(\"üß† Learned Concepts Analysis\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "feature_masks = tabcbm.feature_probabilities.numpy()\n",
        "feature_names = list(X.columns)\n",
        "\n",
        "print(f\"\\\\nüéØ Top 3 Features for Each Concept:\\\\n\")\n",
        "\n",
        "for i in range(N_CONCEPTS):\n",
        "    top_indices = np.argsort(feature_masks[i])[-3:][::-1]\n",
        "    print(f\"   Concept {i:2d}:\")\n",
        "    for rank, idx in enumerate(top_indices, 1):\n",
        "        feat_name = feature_names[idx]\n",
        "        importance = feature_masks[i][idx]\n",
        "        bar = '‚ñà' * int(importance * 15)\n",
        "        print(f\"      {rank}. {feat_name:25s} {importance:.4f} {bar}\")\n",
        "    print()\n",
        "\n",
        "# Concept diversity analysis\n",
        "print(\"\\\\nüìà Concept Diversity Analysis:\")\n",
        "concept_embeddings = np.array([feature_masks[i] for i in range(N_CONCEPTS)])\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "concept_similarity = cosine_similarity(concept_embeddings)\n",
        "avg_similarity = (concept_similarity.sum() - N_CONCEPTS) / (N_CONCEPTS * (N_CONCEPTS - 1))\n",
        "\n",
        "print(f\"   Average inter-concept similarity: {avg_similarity:.4f}\")\n",
        "print(f\"   Concept diversity score: {1-avg_similarity:.4f}\")\n",
        "print(f\"   (Higher diversity = concepts capture different patterns)\")\n",
        "\n",
        "# Most and least similar concept pairs\n",
        "print(f\"\\\\nüîó Concept Relationships:\")\n",
        "sim_pairs = []\n",
        "for i in range(N_CONCEPTS):\n",
        "    for j in range(i+1, N_CONCEPTS):\n",
        "        sim_pairs.append((i, j, concept_similarity[i, j]))\n",
        "\n",
        "sim_pairs.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "print(f\"   Most similar concepts:\")\n",
        "for i, j, sim in sim_pairs[:3]:\n",
        "    print(f\"      Concept {i} ‚Üî Concept {j}: {sim:.4f}\")\n",
        "\n",
        "print(f\"   Most diverse concepts:\")\n",
        "for i, j, sim in sim_pairs[-3:]:\n",
        "    print(f\"      Concept {i} ‚Üî Concept {j}: {sim:.4f}\")\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*70)\n",
        "print(\"üéâ Training Complete!\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "results = {\n",
        "    'test_accuracy': f\"{accuracy:.4f} ({accuracy*100:.2f}%)\",\n",
        "    'n_features': n_features,\n",
        "    'n_concepts': N_CONCEPTS,\n",
        "    'n_classes': n_classes,\n",
        "    'n_train_samples': len(X_train),\n",
        "    'n_test_samples': len(X_test),\n",
        "    'concept_diversity': f\"{1-avg_similarity:.4f}\",\n",
        "    'avg_val_accuracy': f\"{np.mean(history.history.get('val_accuracy', [0])):.4f}\"\n",
        "}\n",
        "\n",
        "print(f\"\\\\nüìù Final Summary:\")\n",
        "for key, value in results.items():\n",
        "    print(f\"   ‚Ä¢ {key}: {value}\")\n",
        "\n",
        "print(f\"\\\\n‚ú® Success! Model is trained and interpretable.\")\n",
        "print(f\"   Each concept focuses on specific features, enabling\")\n",
        "print(f\"   human-understandable explanations for predictions.\")\n",
        "\"\"\"\n",
        "\n",
        "with open('/tmp/train_tabcbm.py', 'w') as f:\n",
        "    f.write(tabcbm_training_script)\n",
        "\n",
        "print(\"‚úÖ FIXED: Handles tuple return from TabCBM\")\n",
        "print(\"üîß Added robust error handling for predictions\")\n",
        "print(\"\\nüöÄ Starting TabCBM training...\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "!conda run -n tabcbm_env python /tmp/train_tabcbm.py"
      ],
      "metadata": {
        "id": "-lY_4ICWC4XW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify feature types\n",
        "target_col = 'target_variable'\n",
        "\n",
        "# Separate features\n",
        "feature_cols = [col for col in df.columns if col in [\"cust_hitrate\", \"cust_interactions\", \"cust_contracts\", \"opp_old\", \"opp_month\", \"product_B_sold_in_the_past\", \"product_A_sold_in_the_past\"]]\n",
        "\n",
        "# Identify categorical and numerical features\n",
        "categorical_features = []\n",
        "numerical_features = []\n",
        "\n",
        "for col in feature_cols:\n",
        "    if df[col].dtype == 'object' or df[col].nunique() < 10:\n",
        "        categorical_features.append(col)\n",
        "    else:\n",
        "        numerical_features.append(col)\n",
        "\n",
        "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
        "print(f\"Numerical features ({len(numerical_features)}): {numerical_features}\")"
      ],
      "metadata": {
        "id": "qxeDvtka9aBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = RandomForestClassifier(\n",
        "    n_estimators=65,\n",
        "    max_depth=35,\n",
        "    random_state=42,\n",
        "    min_samples_split=6,\n",
        "    min_samples_leaf=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate classifier\n",
        "y_pred_train = classifier.predict(X_train)\n",
        "y_pred_test = classifier.predict(X_test)\n",
        "\n",
        "print(f\"\\nTrain Accuracy: {accuracy_score(y_train, y_pred_train):.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_test):.4f}\")\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_test))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "cm = confusion_matrix(y_test, y_pred_test)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_ISWfmCf9qPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TabularDataset(Dataset):\n",
        "    def __init__(self, data, categorical_features, numerical_features, device):\n",
        "        self.data = torch.FloatTensor(data.values)\n",
        "        self.categorical_indices = [data.columns.get_loc(col) for col in categorical_features]\n",
        "        self.numerical_indices = [data.columns.get_loc(col) for col in numerical_features]\n",
        "        self.device = device\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx].to(self.device)"
      ],
      "metadata": {
        "id": "JTkYAxTy90Dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Linear projections\n",
        "        Q = self.W_q(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_k(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        context = torch.matmul(attn, V)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "\n",
        "        output = self.W_o(context)\n",
        "        return output"
      ],
      "metadata": {
        "id": "QYfCsD3q90sP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output = self.attention(x)\n",
        "        x = self.norm1(x + attn_output)\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + ff_output)\n",
        "        return x"
      ],
      "metadata": {
        "id": "0twVjH8W93tZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GumbelSoftmax(nn.Module):\n",
        "    \"\"\"Gumbel-Softmax for differentiable categorical sampling\"\"\"\n",
        "    def __init__(self, tau=1.0, hard=False):\n",
        "        super().__init__()\n",
        "        self.tau = tau\n",
        "        self.hard = hard\n",
        "\n",
        "    def forward(self, logits):\n",
        "        return F.gumbel_softmax(logits, tau=self.tau, hard=self.hard, dim=-1)"
      ],
      "metadata": {
        "id": "tJ6Bsvlk95kM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerVAE(nn.Module):\n",
        "    \"\"\"Transformer-based VAE for tabular data\"\"\"\n",
        "    def __init__(self, input_dim, categorical_indices, numerical_indices,\n",
        "                 categorical_dims, latent_dim=128, d_model=256, num_heads=4,\n",
        "                 num_layers=3, d_ff=512, dropout=0.1, tau=1.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.categorical_indices = categorical_indices\n",
        "        self.numerical_indices = numerical_indices\n",
        "        self.categorical_dims = categorical_dims\n",
        "        self.tau = tau\n",
        "\n",
        "        # Feature embedding\n",
        "        self.feature_embedding = nn.Linear(1, d_model)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.position_embedding = nn.Parameter(torch.randn(1, input_dim, d_model))\n",
        "\n",
        "        # Encoder - Transformer blocks\n",
        "        self.encoder_blocks = nn.ModuleList([\n",
        "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Latent space projection\n",
        "        self.fc_mu = nn.Linear(d_model * input_dim, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(d_model * input_dim, latent_dim)\n",
        "\n",
        "        # Decoder - from latent to features\n",
        "        self.decoder_input = nn.Linear(latent_dim, d_model * input_dim)\n",
        "\n",
        "        # Decoder transformer blocks\n",
        "        self.decoder_blocks = nn.ModuleList([\n",
        "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Output heads for numerical features\n",
        "        self.numerical_head = nn.Linear(d_model, 1)\n",
        "\n",
        "        # Output heads for categorical features (Gumbel-Softmax)\n",
        "        self.categorical_heads = nn.ModuleList([\n",
        "            nn.Linear(d_model, num_classes) for num_classes in categorical_dims\n",
        "        ])\n",
        "        self.gumbel_softmax = GumbelSoftmax(tau=tau, hard=False)\n",
        "\n",
        "    def encode(self, x):\n",
        "        # x shape: (batch_size, input_dim)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Embed each feature\n",
        "        x = x.unsqueeze(-1)  # (batch_size, input_dim, 1)\n",
        "        x = self.feature_embedding(x)  # (batch_size, input_dim, d_model)\n",
        "\n",
        "        # Add positional encoding\n",
        "        x = x + self.position_embedding\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        for block in self.encoder_blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # Flatten and project to latent space\n",
        "        x = x.view(batch_size, -1)\n",
        "        mu = self.fc_mu(x)\n",
        "        logvar = self.fc_logvar(x)\n",
        "\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        # z shape: (batch_size, latent_dim)\n",
        "        batch_size = z.size(0)\n",
        "\n",
        "        # Project from latent space\n",
        "        x = self.decoder_input(z)\n",
        "        x = x.view(batch_size, self.input_dim, -1)\n",
        "\n",
        "        # Pass through decoder transformer blocks\n",
        "        for block in self.decoder_blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # Reconstruct features\n",
        "        reconstructed = torch.zeros(batch_size, self.input_dim).to(z.device)\n",
        "\n",
        "        # Numerical features\n",
        "        for idx in self.numerical_indices:\n",
        "            reconstructed[:, idx] = self.numerical_head(x[:, idx]).squeeze(-1)\n",
        "\n",
        "        # Categorical features with Gumbel-Softmax\n",
        "        for cat_idx, head in enumerate(self.categorical_heads):\n",
        "            feat_idx = self.categorical_indices[cat_idx]\n",
        "            logits = head(x[:, feat_idx])\n",
        "            # Sample from Gumbel-Softmax\n",
        "            probs = self.gumbel_softmax(logits)\n",
        "            # Convert back to class index (soft argmax)\n",
        "            class_indices = torch.sum(probs * torch.arange(probs.size(-1)).float().to(z.device), dim=-1)\n",
        "            reconstructed[:, feat_idx] = class_indices\n",
        "\n",
        "        return reconstructed\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_recon = self.decode(z)\n",
        "        return x_recon, mu, logvar\n",
        "\n",
        "    def encode_to_latent(self, x):\n",
        "        \"\"\"Encode input to latent representation (deterministic)\"\"\"\n",
        "        mu, _ = self.encode(x)\n",
        "        return mu"
      ],
      "metadata": {
        "id": "cqnQ_VHd96HF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vae_loss_function(x_recon, x, mu, logvar, categorical_indices, numerical_indices, beta=1.0):\n",
        "    \"\"\"VAE loss with separate reconstruction losses for numerical and categorical features\"\"\"\n",
        "\n",
        "    # Reconstruction loss for numerical features (MSE)\n",
        "    num_recon_loss = F.mse_loss(\n",
        "        x_recon[:, numerical_indices],\n",
        "        x[:, numerical_indices],\n",
        "        reduction='sum'\n",
        "    )\n",
        "\n",
        "    # Reconstruction loss for categorical features (cross-entropy)\n",
        "    cat_recon_loss = 0\n",
        "    for idx in categorical_indices:\n",
        "        # Round to nearest integer for categorical\n",
        "        cat_recon_loss += F.mse_loss(\n",
        "            x_recon[:, idx],\n",
        "            x[:, idx],\n",
        "            reduction='sum'\n",
        "        )\n",
        "\n",
        "    # KL divergence\n",
        "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    # Total loss\n",
        "    total_loss = num_recon_loss + cat_recon_loss + beta * kld\n",
        "\n",
        "    return total_loss, num_recon_loss, cat_recon_loss, kld"
      ],
      "metadata": {
        "id": "1KlK3v449-PY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize VAE\n",
        "vae = TransformerVAE(\n",
        "    input_dim=len(feature_cols),\n",
        "    categorical_indices=categorical_indices,\n",
        "    numerical_indices=numerical_indices,\n",
        "    categorical_dims=categorical_dims,\n",
        "    latent_dim=64,\n",
        "    d_model=128,\n",
        "    num_heads=4,\n",
        "    num_layers=2,\n",
        "    d_ff=256,\n",
        "    dropout=0.1,\n",
        "    tau=0.5\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model parameters: {sum(p.numel() for p in vae.parameters()):,}\")"
      ],
      "metadata": {
        "id": "EVt8B9Y_-V80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets and dataloaders\n",
        "train_dataset = TabularDataset(X_train, categorical_features, numerical_features, device)\n",
        "test_dataset = TabularDataset(X_test, categorical_features, numerical_features, device)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)"
      ],
      "metadata": {
        "id": "tc1mpWxU-dM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 100\n",
        "beta = 0.5  # KL divergence weight\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "print(\"Starting VAE training...\")\n",
        "for epoch in range(num_epochs):\n",
        "    vae.train()\n",
        "    train_loss = 0\n",
        "    train_recon_loss = 0\n",
        "    train_kld_loss = 0\n",
        "\n",
        "    for batch_idx, data in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x_recon, mu, logvar = vae(data)\n",
        "        loss, num_loss, cat_loss, kld = vae_loss_function(\n",
        "            x_recon, data, mu, logvar,\n",
        "            categorical_indices, numerical_indices, beta\n",
        "        )\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_recon_loss += (num_loss.item() + cat_loss.item())\n",
        "        train_kld_loss += kld.item()\n",
        "\n",
        "    # Validation\n",
        "    vae.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            data = data.to(device)\n",
        "            x_recon, mu, logvar = vae(data)\n",
        "            loss, _, _, _ = vae_loss_function(\n",
        "                x_recon, data, mu, logvar,\n",
        "                categorical_indices, numerical_indices, beta\n",
        "            )\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
        "    avg_test_loss = test_loss / len(test_loader.dataset)\n",
        "\n",
        "    train_losses.append(avg_train_loss)\n",
        "    test_losses.append(avg_test_loss)\n",
        "\n",
        "    scheduler.step(avg_test_loss)\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
        "              f'Train Loss: {avg_train_loss:.4f}, '\n",
        "              f'Test Loss: {avg_test_loss:.4f}, '\n",
        "              f'Recon: {train_recon_loss/len(train_loader.dataset):.4f}, '\n",
        "              f'KLD: {train_kld_loss/len(train_loader.dataset):.4f}')\n",
        "\n",
        "print(\"\\nTraining completed!\")"
      ],
      "metadata": {
        "id": "atEc5I3E-gO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select test instances where prediction is negative (class 0)\n",
        "test_predictions = classifier.predict(X_test)\n",
        "negative_instances = X_test[test_predictions == 0]\n",
        "\n",
        "print(f\"Found {len(negative_instances)} instances predicted as class 0\")\n",
        "print(f\"We will generate counterfactuals to flip them to class 1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "QNN5SEbu-852",
        "outputId": "1426d748-9550-46fe-e100-f700cae550be"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'classifier' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-697809032.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Select test instances where prediction is negative (class 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnegative_instances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_predictions\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Found {len(negative_instances)} instances predicted as class 0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STANDALONE IMPROVED TABCF COUNTERFACTUAL GENERATOR\n",
        "# Complete, self-contained implementation\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ============================================================\n",
        "# COMPLETE COUNTERFACTUAL GENERATOR CLASS\n",
        "# ============================================================\n",
        "\n",
        "class StandaloneCounterfactualGenerator:\n",
        "    \"\"\"\n",
        "    Complete counterfactual generator with all improvements built-in.\n",
        "    No dependencies on previous implementations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vae, classifier, feature_cols, categorical_features,\n",
        "                 numerical_features, scaler, device, immutable_features=None):\n",
        "        self.vae = vae\n",
        "        self.classifier = classifier\n",
        "        self.feature_cols = feature_cols\n",
        "        self.categorical_features = categorical_features\n",
        "        self.numerical_features = numerical_features\n",
        "        self.scaler = scaler\n",
        "        self.device = device\n",
        "\n",
        "        # Features that shouldn't change\n",
        "        self.immutable_features = immutable_features if immutable_features else []\n",
        "        self.immutable_indices = [i for i, feat in enumerate(feature_cols)\n",
        "                                  if feat in self.immutable_features]\n",
        "\n",
        "    def generate_counterfactual_progressive(self, x_original, target_class,\n",
        "                                           num_iterations=1500,\n",
        "                                           learning_rate=0.01,\n",
        "                                           verbose=False):\n",
        "        \"\"\"\n",
        "        Progressive optimization with 3 phases.\n",
        "        \"\"\"\n",
        "        self.vae.eval()\n",
        "\n",
        "        # Encode to latent space\n",
        "        x_tensor = torch.FloatTensor(x_original).unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            mu, logvar = self.vae.encode(x_tensor)\n",
        "            z_original = mu\n",
        "\n",
        "        z = z_original.clone().detach().requires_grad_(True)\n",
        "        optimizer = torch.optim.Adam([z], lr=learning_rate)\n",
        "\n",
        "        best_valid_z = None\n",
        "        best_valid_loss = float('inf')\n",
        "        found_valid = False\n",
        "\n",
        "        for iteration in range(num_iterations):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Decode\n",
        "            x_cf_tensor = self.vae.decode(z)\n",
        "            x_cf = x_cf_tensor.detach().cpu().numpy()[0]\n",
        "\n",
        "            # Predict\n",
        "            pred_proba = self.classifier.predict_proba(x_cf.reshape(1, -1))[0]\n",
        "            pred_class = np.argmax(pred_proba)\n",
        "            confidence = pred_proba[target_class]\n",
        "\n",
        "            # Progressive weights\n",
        "            if iteration < 500:\n",
        "                lambda_validity = 50.0\n",
        "                lambda_proximity = 0.1\n",
        "                lambda_sparsity = 0.01\n",
        "            elif iteration < 1000:\n",
        "                lambda_validity = 20.0\n",
        "                lambda_proximity = 0.3\n",
        "                lambda_sparsity = 2.0\n",
        "            else:\n",
        "                lambda_validity = 30.0\n",
        "                lambda_proximity = 0.5\n",
        "                lambda_sparsity = 5.0\n",
        "\n",
        "            # Loss components\n",
        "            validity_loss = torch.max(\n",
        "                torch.tensor(0.0, device=self.device),\n",
        "                torch.tensor(0.7, device=self.device) - torch.tensor(confidence, device=self.device)\n",
        "            )\n",
        "\n",
        "            proximity_loss = torch.norm(z - z_original, p=2)\n",
        "\n",
        "            x_diff = torch.abs(x_cf_tensor - x_tensor)\n",
        "            threshold = 0.1\n",
        "            feature_changed = (x_diff > threshold).float()\n",
        "            sparsity_loss = torch.sum(feature_changed) + 0.1 * torch.sum(x_diff)\n",
        "\n",
        "            # Immutability\n",
        "            immutability_loss = torch.tensor(0.0, device=self.device)\n",
        "            for idx in self.immutable_indices:\n",
        "                immutability_loss += 1000.0 * torch.abs(x_cf_tensor[0, idx] - x_tensor[0, idx])\n",
        "\n",
        "            total_loss = (lambda_validity * validity_loss +\n",
        "                         lambda_proximity * proximity_loss +\n",
        "                         lambda_sparsity * sparsity_loss +\n",
        "                         immutability_loss)\n",
        "\n",
        "            total_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_([z], max_norm=0.5)\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track best valid solution\n",
        "            if pred_class == target_class:\n",
        "                if not found_valid or total_loss.item() < best_valid_loss:\n",
        "                    best_valid_loss = total_loss.item()\n",
        "                    best_valid_z = z.clone()\n",
        "                    found_valid = True\n",
        "\n",
        "                if confidence > 0.75 and torch.sum(feature_changed).item() < 5:\n",
        "                    if verbose:\n",
        "                        print(f\"Excellent CF at iteration {iteration}\")\n",
        "                    break\n",
        "\n",
        "        final_z = best_valid_z if found_valid else z\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x_cf_final = self.vae.decode(final_z).cpu().numpy()[0]\n",
        "\n",
        "        return x_cf_final, found_valid\n",
        "\n",
        "    def generate_counterfactual_greedy(self, x_original, target_class, max_features=5):\n",
        "        \"\"\"\n",
        "        Greedy approach: change features one at a time.\n",
        "        \"\"\"\n",
        "        x_cf = x_original.copy()\n",
        "        pred_original = self.classifier.predict_proba(x_original.reshape(1, -1))[0]\n",
        "\n",
        "        feature_scores = []\n",
        "\n",
        "        for i, feat in enumerate(self.feature_cols):\n",
        "            if feat in self.immutable_features:\n",
        "                continue\n",
        "\n",
        "            x_test = x_original.copy()\n",
        "\n",
        "            if feat in self.categorical_features:\n",
        "                unique_vals = [0, 1] if x_original[i] in [0, 1] else [0, 1, 2]\n",
        "                for val in unique_vals:\n",
        "                    if val != x_original[i]:\n",
        "                        x_test[i] = val\n",
        "                        pred = self.classifier.predict_proba(x_test.reshape(1, -1))[0]\n",
        "                        improvement = pred[target_class] - pred_original[target_class]\n",
        "                        feature_scores.append((i, val, improvement))\n",
        "            else:\n",
        "                x_test[i] = x_original[i] + 1.0\n",
        "                pred_plus = self.classifier.predict_proba(x_test.reshape(1, -1))[0]\n",
        "\n",
        "                x_test[i] = x_original[i] - 1.0\n",
        "                pred_minus = self.classifier.predict_proba(x_test.reshape(1, -1))[0]\n",
        "\n",
        "                improvement_plus = pred_plus[target_class] - pred_original[target_class]\n",
        "                improvement_minus = pred_minus[target_class] - pred_original[target_class]\n",
        "\n",
        "                if improvement_plus > 0:\n",
        "                    feature_scores.append((i, x_original[i] + 1.0, improvement_plus))\n",
        "                if improvement_minus > 0:\n",
        "                    feature_scores.append((i, x_original[i] - 1.0, improvement_minus))\n",
        "\n",
        "        feature_scores.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "        changed_features = 0\n",
        "        for i, new_val, improvement in feature_scores:\n",
        "            if changed_features >= max_features:\n",
        "                break\n",
        "\n",
        "            x_cf[i] = new_val\n",
        "            changed_features += 1\n",
        "\n",
        "            pred = self.classifier.predict_proba(x_cf.reshape(1, -1))[0]\n",
        "            if np.argmax(pred) == target_class and pred[target_class] > 0.6:\n",
        "                break\n",
        "\n",
        "        return x_cf\n",
        "\n",
        "    def generate_counterfactual_hybrid(self, x_original, target_class):\n",
        "        \"\"\"\n",
        "        Hybrid: Try greedy first, fall back to progressive if needed.\n",
        "        \"\"\"\n",
        "        # Try greedy\n",
        "        x_cf_greedy = self.generate_counterfactual_greedy(\n",
        "            x_original, target_class, max_features=5\n",
        "        )\n",
        "\n",
        "        metrics_greedy = self.evaluate_counterfactual(\n",
        "            x_original, x_cf_greedy, target_class\n",
        "        )\n",
        "\n",
        "        # If greedy worked well, use it\n",
        "        if metrics_greedy['is_valid'] and metrics_greedy['changed_features'] <= 5:\n",
        "            return x_cf_greedy, metrics_greedy\n",
        "\n",
        "        # Otherwise, use progressive\n",
        "        x_cf_progressive, found_valid = self.generate_counterfactual_progressive(\n",
        "            x_original, target_class\n",
        "        )\n",
        "\n",
        "        metrics_progressive = self.evaluate_counterfactual(\n",
        "            x_original, x_cf_progressive, target_class\n",
        "        )\n",
        "\n",
        "        # Return better result\n",
        "        if metrics_greedy['is_valid'] and metrics_progressive['is_valid']:\n",
        "            if metrics_greedy['changed_features'] <= metrics_progressive['changed_features']:\n",
        "                return x_cf_greedy, metrics_greedy\n",
        "            else:\n",
        "                return x_cf_progressive, metrics_progressive\n",
        "        elif metrics_progressive['is_valid']:\n",
        "            return x_cf_progressive, metrics_progressive\n",
        "        else:\n",
        "            return x_cf_greedy, metrics_greedy\n",
        "\n",
        "    def evaluate_counterfactual(self, x_original, x_cf, target_class):\n",
        "        \"\"\"\n",
        "        Evaluate counterfactual quality.\n",
        "        \"\"\"\n",
        "        pred_proba = self.classifier.predict_proba(x_cf.reshape(1, -1))[0]\n",
        "        predicted_class = np.argmax(pred_proba)\n",
        "        confidence = pred_proba[target_class]\n",
        "        is_valid = (predicted_class == target_class)\n",
        "\n",
        "        proximity = np.linalg.norm(x_original - x_cf)\n",
        "\n",
        "        tolerance = 1e-3\n",
        "        changed_features = np.sum(np.abs(x_original - x_cf) > tolerance)\n",
        "        sparsity = 1 - (changed_features / len(x_original))\n",
        "\n",
        "        return {\n",
        "            'is_valid': is_valid,\n",
        "            'predicted_class': predicted_class,\n",
        "            'confidence': confidence,\n",
        "            'proximity': proximity,\n",
        "            'changed_features': changed_features,\n",
        "            'sparsity': sparsity\n",
        "        }\n",
        "\n",
        "\n",
        "def postprocess_counterfactual(x_cf, x_original, feature_cols,\n",
        "                               categorical_features, immutable_features):\n",
        "    \"\"\"\n",
        "    Post-process to enforce constraints.\n",
        "    \"\"\"\n",
        "    x_cf_processed = x_cf.copy()\n",
        "\n",
        "    # Enforce immutable features\n",
        "    for feat in immutable_features:\n",
        "        if feat in feature_cols:\n",
        "            idx = feature_cols.index(feat)\n",
        "            x_cf_processed[idx] = x_original[idx]\n",
        "\n",
        "    # Round categorical features\n",
        "    for feat in categorical_features:\n",
        "        if feat in feature_cols:\n",
        "            idx = feature_cols.index(feat)\n",
        "            x_cf_processed[idx] = np.round(x_cf_processed[idx])\n",
        "\n",
        "    return x_cf_processed\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# USAGE: GENERATE COUNTERFACTUALS\n",
        "# ============================================================\n",
        "\n",
        "# Define immutable features (adjust for your dataset)\n",
        "immutable_features = ['age', 'race', 'sex', 'native-country']\n",
        "\n",
        "# Initialize generator\n",
        "cf_generator = StandaloneCounterfactualGenerator(\n",
        "    vae=vae,\n",
        "    classifier=classifier,\n",
        "    feature_cols=feature_cols,\n",
        "    categorical_features=categorical_features,\n",
        "    numerical_features=numerical_features,\n",
        "    scaler=scaler,\n",
        "    device=device,\n",
        "    immutable_features=immutable_features\n",
        ")\n",
        "\n",
        "# Generate counterfactuals\n",
        "print(\"Generating counterfactuals with standalone hybrid approach...\")\n",
        "print(\"This combines greedy (fast, sparse) and progressive (high validity) methods.\\n\")\n",
        "\n",
        "final_counterfactuals = []\n",
        "final_metrics_list = []\n",
        "\n",
        "for idx in tqdm(sample_indices, desc=\"Generating CFs\"):\n",
        "    x_original = negative_instances.iloc[idx].values\n",
        "\n",
        "    # Generate using hybrid approach\n",
        "    x_cf, metrics = cf_generator.generate_counterfactual_hybrid(\n",
        "        x_original,\n",
        "        target_class=1\n",
        "    )\n",
        "\n",
        "    # Post-process\n",
        "    x_cf = postprocess_counterfactual(\n",
        "        x_cf, x_original, feature_cols,\n",
        "        categorical_features, immutable_features\n",
        "    )\n",
        "\n",
        "    # Re-evaluate after post-processing\n",
        "    final_metrics = cf_generator.evaluate_counterfactual(\n",
        "        x_original, x_cf, target_class=1\n",
        "    )\n",
        "\n",
        "    final_counterfactuals.append({\n",
        "        'original': x_original,\n",
        "        'counterfactual': x_cf\n",
        "    })\n",
        "    final_metrics_list.append(final_metrics)\n",
        "\n",
        "# ============================================================\n",
        "# PRINT RESULTS\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINAL COUNTERFACTUAL EVALUATION METRICS\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "metrics_df_final = pd.DataFrame(final_metrics_list)\n",
        "\n",
        "validity_rate = metrics_df_final['is_valid'].mean() * 100\n",
        "avg_confidence = metrics_df_final['confidence'].mean()\n",
        "avg_proximity = metrics_df_final['proximity'].mean()\n",
        "avg_changed = metrics_df_final['changed_features'].mean()\n",
        "avg_sparsity = metrics_df_final['sparsity'].mean() * 100\n",
        "\n",
        "print(f\"Validity Rate:           {validity_rate:.2f}%\")\n",
        "print(f\"Average Confidence:      {avg_confidence:.4f}\")\n",
        "print(f\"Average Proximity:       {avg_proximity:.4f}\")\n",
        "print(f\"Average Changed Features: {avg_changed:.2f}\")\n",
        "print(f\"Average Sparsity:        {avg_sparsity:.2f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"COMPARISON WITH ORIGINAL RESULTS\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "print(f\"{'Metric':<30} {'Original':<15} {'New':<15} {'Change':<15}\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'Validity Rate (%)':<30} {40.00:<15.2f} {validity_rate:<15.2f} {validity_rate-40.00:+.2f}\")\n",
        "print(f\"{'Average Confidence':<30} {0.5411:<15.4f} {avg_confidence:<15.4f} {avg_confidence-0.5411:+.4f}\")\n",
        "print(f\"{'Average Proximity':<30} {0.9720:<15.4f} {avg_proximity:<15.4f} {avg_proximity-0.9720:+.4f}\")\n",
        "print(f\"{'Changed Features':<30} {7.90:<15.2f} {avg_changed:<15.2f} {avg_changed-7.90:+.2f}\")\n",
        "print(f\"{'Sparsity (%)':<30} {52.67:<15.2f} {avg_sparsity:<15.2f} {avg_sparsity-52.67:+.2f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DETAILED STATISTICS\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "print(metrics_df_final.describe())\n",
        "\n",
        "# ============================================================\n",
        "# VISUALIZE IMPROVEMENTS\n",
        "# ============================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Validity comparison\n",
        "axes[0, 0].bar(['Original\\n(40%)', 'Improved\\n(60%)', f'Final\\n({validity_rate:.1f}%)'],\n",
        "               [40, 60, validity_rate],\n",
        "               color=['#ff6b6b', '#ffd93d', '#6bcf7f'])\n",
        "axes[0, 0].set_title('Validity Rate Comparison', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_ylabel('Validity Rate (%)')\n",
        "axes[0, 0].axhline(y=70, color='green', linestyle='--', label='Target (70%)')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].set_ylim(0, 100)\n",
        "\n",
        "# Changed features comparison\n",
        "axes[0, 1].bar(['Original\\n(7.9)', 'Improved\\n(10.05)', f'Final\\n({avg_changed:.1f})'],\n",
        "               [7.9, 10.05, avg_changed],\n",
        "               color=['#ff6b6b', '#ffd93d', '#6bcf7f'])\n",
        "axes[0, 1].set_title('Average Changed Features', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_ylabel('Number of Features')\n",
        "axes[0, 1].axhline(y=5, color='green', linestyle='--', label='Target (<5)')\n",
        "axes[0, 1].legend()\n",
        "\n",
        "# Sparsity comparison\n",
        "axes[1, 0].bar(['Original\\n(52.67%)', 'Improved\\n(33%)', f'Final\\n({avg_sparsity:.1f}%)'],\n",
        "               [52.67, 33, avg_sparsity],\n",
        "               color=['#ff6b6b', '#ffd93d', '#6bcf7f'])\n",
        "axes[1, 0].set_title('Sparsity Comparison', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_ylabel('Sparsity (%)')\n",
        "axes[1, 0].axhline(y=70, color='green', linestyle='--', label='Target (>70%)')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].set_ylim(0, 100)\n",
        "\n",
        "# Distribution of changed features\n",
        "axes[1, 1].hist(metrics_df_final['changed_features'], bins=range(0, int(metrics_df_final['changed_features'].max())+2),\n",
        "                edgecolor='black', alpha=0.7, color='#6bcf7f')\n",
        "axes[1, 1].set_title('Distribution of Changed Features', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Number of Changed Features')\n",
        "axes[1, 1].set_ylabel('Count')\n",
        "axes[1, 1].axvline(x=5, color='red', linestyle='--', label='Target (<5)')\n",
        "axes[1, 1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Counterfactual generation complete!\")\n",
        "print(f\"üìä {len([m for m in final_metrics_list if m['is_valid']])} out of {len(final_metrics_list)} counterfactuals are valid\")\n",
        "print(f\"üéØ Average of {avg_changed:.2f} features changed per counterfactual\")"
      ],
      "metadata": {
        "id": "3cSQXGo5-pu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_counterfactual(idx, counterfactuals, metrics_list, feature_cols,\n",
        "                            numerical_features, categorical_features):\n",
        "    \"\"\"\n",
        "    Visualize a specific counterfactual example.\n",
        "    \"\"\"\n",
        "    cf = counterfactuals[idx]\n",
        "    metrics = metrics_list[idx]\n",
        "\n",
        "    x_orig = cf['original']\n",
        "    x_cf = cf['counterfactual']\n",
        "\n",
        "    # Calculate changes\n",
        "    changes = x_cf - x_orig\n",
        "    significant_changes = np.abs(changes) > 0.1\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"COUNTERFACTUAL EXAMPLE #{idx+1}\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nValidity: {'‚úì Valid' if metrics['is_valid'] else '‚úó Invalid'}\")\n",
        "    print(f\"Predicted Class: {metrics['predicted_class']} (Confidence: {metrics['confidence']:.4f})\")\n",
        "    print(f\"Proximity: {metrics['proximity']:.4f}\")\n",
        "    print(f\"Changed Features: {metrics['changed_features']} ({metrics['sparsity']:.2%} of total)\")\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"FEATURE CHANGES:\")\n",
        "    print(\"-\"*80)\n",
        "    print(f\"{'Feature':<30} {'Original':<15} {'Counterfactual':<15} {'Change':<15}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    for i, feature in enumerate(feature_cols):\n",
        "        if significant_changes[i]:\n",
        "            marker = \"*\" if feature in categorical_features else \" \"\n",
        "            print(f\"{marker}{feature:<29} {x_orig[i]:<15.4f} {x_cf[i]:<15.4f} {changes[i]:<15.4f}\")\n",
        "\n",
        "    print(\"-\"*80)\n",
        "    print(\"* indicates categorical feature\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Visualization\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # Plot 1: Feature comparison\n",
        "    changed_indices = np.where(significant_changes)[0]\n",
        "    if len(changed_indices) > 0:\n",
        "        x_pos = np.arange(len(changed_indices))\n",
        "        width = 0.35\n",
        "\n",
        "        axes[0].bar(x_pos - width/2, x_orig[changed_indices], width,\n",
        "                   label='Original', alpha=0.8)\n",
        "        axes[0].bar(x_pos + width/2, x_cf[changed_indices], width,\n",
        "                   label='Counterfactual', alpha=0.8)\n",
        "        axes[0].set_xlabel('Changed Features')\n",
        "        axes[0].set_ylabel('Feature Value')\n",
        "        axes[0].set_title('Feature Value Comparison')\n",
        "        axes[0].set_xticks(x_pos)\n",
        "        axes[0].set_xticklabels([feature_cols[i] for i in changed_indices],\n",
        "                                rotation=45, ha='right')\n",
        "        axes[0].legend()\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: Feature changes magnitude\n",
        "    axes[1].barh(range(len(feature_cols)), np.abs(changes))\n",
        "    axes[1].set_yticks(range(len(feature_cols)))\n",
        "    axes[1].set_yticklabels(feature_cols)\n",
        "    axes[1].set_xlabel('Absolute Change')\n",
        "    axes[1].set_title('Feature Change Magnitude')\n",
        "    axes[1].axvline(0.1, color='red', linestyle='--', alpha=0.5,\n",
        "                   label='Significance Threshold')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "VvOQW5DW_RI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize a few examples\n",
        "num_examples = min(3, len(final_counterfactuals))\n",
        "for i in range(num_examples):\n",
        "    visualize_counterfactual(i, final_counterfactuals, final_metrics_list, feature_cols,\n",
        "                            numerical_features, categorical_features)"
      ],
      "metadata": {
        "id": "rJvZ8Hdw-1mp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}